# Depthwise Separable Convolution

我们先回顾一下前几节中讨论的2D卷积和1 x 1卷积。让我们快速回顾一下标准2D卷积。举一个具体的例子，假设输入层的大小为7 x 7 x 3（高度x宽度x通道），而filter的大小为3 x 3 x3。用一个filter进行2D卷积后，输出层为大小为5 x 5 x 1（只有1个通道）。

![](../../../.gitbook/assets/image%20%28131%29.png)

通常，在两个神经网络层之间应用多个过滤器。假设这里有128个过滤器。应用这128个2D卷积后，我们得到128个5 x 5 x 1的特征图。然后，我们将这些地图堆叠到大小为5 x 5 x 128的单层中。这样做，我们将输入层（7 x 7 x 3）转换为输出层（5 x 5 x 128）。在扩展深度的同时，缩小了空间尺寸，即高度和宽度。

![](../../../.gitbook/assets/image%20%28130%29.png)

现在，通过深度可分离卷积，让我们看看如何实现相同的变换。

首先，我们将深度卷积应用于输入层。我们没有在2D卷积中使用单个大小为3 x 3 x 3的滤波器，而是分别使用了3个内核。每个filter的大小为3 x 3 x1。每个内核与输入层的1个通道卷积（仅1个通道，而不是所有通道！）。每个这样的卷积都提供大小为5 x 5 x 1的特征图。然后，我们将这些特征图堆叠在一起以创建5 x 5 x 3的图像。此后，我们得到大小为5 x 5 x 3的输出。我们现在缩小空间尺寸，但深度仍与以前相同。

![](../../../.gitbook/assets/image%20%28120%29.png)

作为深度可分离卷积的第二步，为了扩展深度，我们应用内核大小为1x1x3的1x1卷积。将5 x 5 x 3输入图像与每个1 x 1 x 3内核进行卷积可得到大小为5 x 5 x 1的特征图。

![](../../../.gitbook/assets/image%20%28124%29.png)

因此，应用128个1x1卷积后，我们可以得到一个大小为5 x 5 x 128的图层。

![](../../../.gitbook/assets/image%20%28114%29.png)

通过这两个步骤，深度可分离卷积还将输入层（7 x 7 x 3）转换为输出层（5 x 5 x 128）。

下图显示了深度可分离卷积的整个过程。

![](../../../.gitbook/assets/image%20%28126%29.png)

那么，进行深度可分离卷积有什么好处？效率！与2D卷积相比，深度可分离卷积所需的运算量少得多。

让我们回想一下2D卷积示例的计算成本。有128个3x3x3内核运行5x5次。那就是128 x 3 x 3 x 3 x 5 x 5 = 86,400乘法。

可分离的卷积怎么样？在第一个深度卷积步骤中，有3个3x3x1内核移动5x5次。那就是3x3x3x1x5x5 = 675乘法。在1 x 1卷积的第二步中，有128个1x1x3内核移动5x5次。那就是128 x 1 x 1 x 3 x 5 x 5 = 9,600乘法。因此，总体上，深度可分离卷积需要675 + 9600 = 10,275次。这仅是2D卷积成本的12％左右！

因此，对于任意大小的图像，如果应用深度可分离卷积可以节省多少时间。让我们对以上示例进行一些概括。现在，对于大小为H xW x D的输入图像，我们希望使用大小为h x h x D的$$N_c$$内核进行2D卷积（步幅= 1，填充= 0），其中h为偶数。这将输入层（H xW x D）转换为输出层（H-h + 1 x W-h + 1 x Nc）。

深度可分离卷积和2D卷积之间的乘法比为：

$$
\frac{1}{N_c} = \frac{1}{h^2}
$$

对于大多数现代架构，通常输出层具有许多通道，例如几百甚至几千。对于此类层（Nc &gt;&gt; h），则上述表达式可降低到$$\frac{1}{h^2}$$。这意味着对于这种渐近表达式，如果使用3 x 3滤镜，则2D卷积的乘积是深度可分离卷积的9倍。对于5 x 5滤镜，2D卷积的乘法运算要多25倍。

使用深度可分离卷积有什么缺点吗？当然可以。深度可分离卷积减少了卷积中的参数数量。这样，对于小型模型，如果将2D卷积替换为深度可分离卷积，则模型容量可能会大大降低。结果，模型可能变得不理想。但是，如果使用得当，则深度可分离卷积可以在不显着损害模​​型性能的情况下提高效率。

