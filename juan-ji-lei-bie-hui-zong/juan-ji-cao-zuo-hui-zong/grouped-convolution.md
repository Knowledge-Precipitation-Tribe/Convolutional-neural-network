# Grouped Convolution

分组卷积在2012年的AlexNet论文（[链接](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)）中引入。实现它的主要原因是允许在内存有限（每个GPU 1.5 GB内存）的两个GPU上进行网络训练。下面的AlexNet在大多数层上显示了两个单独的卷积路径。它正在两个GPU之间进行模型并行化（当然，如果有更多GPU可用，则可以并行执行多个GPU）。

![](../../.gitbook/assets/image%20%28134%29.png)

在这里，我们描述分组卷积如何工作。首先，常规2D卷积遵循以下所示的步骤。在此示例中，通过应用128个filter（每个filter的尺寸为3 x 3 x 3），将尺寸为（7 x 7 x 3）的输入层转换为尺寸为（5 x 5 x 128）的输出层。或通常情况下，通过应用Dout个filter将输入大小（Hin x Win x Din）的输入层转换为大小（Hout x Wout x Dout）的输出层（每个大小为h x w x Din）。

![](../../.gitbook/assets/image%20%28128%29.png)

在分组卷积中，filter分为不同的组。每个组负责一定深度的常规2D卷积。以下示例可以使这一点更加清楚。

![](../../.gitbook/assets/image%20%28123%29.png)

上面是带有2个filter组的分组卷积的说明。在每个滤波器组中，每个滤波器的深度仅为标称2D卷积的深度的一半。它们的深度为Din / 2。每个过滤器组包含Dout / 2过滤器。第一个滤镜组（黄色）与输入层的上半部分（\[：，：，0：Din / 2\]）卷积，而第二个滤镜组（红色）与输入层的下半部分（\[：，：，Din / 2：Din\]）进行卷积。结果，每个过滤器组都会创建Dout / 2通道。总体而言，两组创建2个Dout / 2 = Dout通道。然后，将这些通道与Dout通道堆叠在输出层中。

## Grouped convolution v.s. depthwise convolution

您可能已经观察到分组卷积和深度可分离卷积中使用的深度卷积之间的某些联系和差异。如果filter组的数量与输入层通道的数量相同，则每个滤镜的深度Din / Din =1。这与深度卷积中的滤镜深度相同。

另一方面，每个过滤器组现在都包含Dout / Din过滤器。总体而言，输出层的深度为Dout。这与深度卷积法不同，后者不会改变层深度。层深度随后在深度可分离卷积中以1x1卷积扩展。

## advantages

第一个优势是有效的训练。由于卷积被分为几个路径，因此每个路径可以由不同的GPU分别处理。此过程允许以并行方式在多个GPU上进行模型训练。与使用一个GPU进行所有训练相比，通过多GPU进行的模型并行化允许每步将更多图像馈入网络。模型并行化被认为比数据并行化更好。后面的一个将数据集分成多个批次，然后我们对每个批次进行训练。但是，当批次大小变得太小时，与批次梯度下降相比，我们实质上是随机的。这将导致收敛变慢，有时甚至变差。

分组卷积对于训练非常深的神经网络非常重要，如下面的ResNeXt所示

![](../../.gitbook/assets/image%20%28136%29.png)

第二个优点是模型效率更高，即模型参数随着过滤器组数的增加而减小。在前面的示例中，filter在标准2D卷积中具有h x w x Din x Dout参数。具有2个filter组的分组卷积中的滤镜具有（h xw x Din / 2 x Dout / 2）x 2个参数。参数数量减少一半。

第三个优势有点令人惊讶。分组卷积可以提供比标准2D卷积更好的模型。这个另一个很棒的博客（[链接](https://blog.yani.io/filter-group-tutorial/)）对此进行了解释。

