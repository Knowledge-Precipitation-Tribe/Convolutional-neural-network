# Transposed Convolution vs Deconvolution

对于许多应用程序和许多网络体系结构，我们通常希望进行与正常卷积相反方向的转换，即我们要执行上采样。一些示例包括生成高分辨率图像以及将低维特征图映射到高维空间，例如在自动编码器或语义分割中。

传统上，可以通过应用插值方案或手动创建规则来实现上采样。另一方面，诸如神经网络之类的现代体系结构倾向于让网络本身自动学习正确的转换，而无需人工干预。为此，我们可以使用转置卷积。

## 转置卷积

转置卷积在文献中也称为解卷积或分数步卷积。但是，值得注意的是，“反卷积”这个名称不太合适，因为转置卷积不是信号/图像处理中定义的真正的反卷积。从技术上讲，信号处理中的反卷积可以逆转卷积运算。这里情况不同。因此，一些作者强烈反对将转置卷积称为反卷积。人们称其为反卷积主要是因为简单性。稍后，我们将了解为什么将这种操作称为转置卷积是自然且更合适的。

始终可以通过直接卷积实现转置卷积。

对于下图中的示例，我们将3×3内核的转置卷积应用到2×2输入上，并使用单位跨度填充2×2的零边界。上采样输出的大小为4 x 4。

![](../../.gitbook/assets/fig2.gif)

有趣的是，可以通过应用花哨的填充和跨距将相同的2 x 2输入图像映射到不同的图像大小。下面，使用单位步幅将转置卷积应用于相同的2 x 2输入（在输入之间插入1个零），并用2 x 2的零边界填充。现在输出的大小为5 x 5。

![](../../.gitbook/assets/fig3.gif)

在以上示例中查看转置卷积可以帮助我们建立一些直觉。但是要概括其应用，查看一下如何通过计算机中的矩阵乘法来实现它是有益的。从那里，我们还可以看到为什么“转置卷积”是一个合适的名称。

在卷积中，让我们将C定义为kernel，将Large定义为输入图像，将Small定义为卷积中的输出图像。卷积（矩阵乘法）后，我们将大图像下采样为小输出图像。矩阵乘法中的卷积实现如下：$$C \times Large=Small$$。

以下示例显示了这种操作的工作方式。它将输入展平为16 x 1矩阵，并将内核转换为稀疏矩阵（4 x 16）。然后在稀疏矩阵和平坦输入之间应用矩阵乘法。然后，将所得矩阵（4 x 1）转换回2 x 2输出。

![](../../.gitbook/assets/image%20%28137%29.png)

现在，如果我们在等式两边乘以矩阵$$C$$的转置$$C^T$$，并利用矩阵与其转置矩阵的相乘得到单位矩阵的性质，则我们有以下公式$$C^T \times Small = Large$$，如下所示：

![](../../.gitbook/assets/image%20%28118%29.png)

如您在此处看到的，我们执行从小图像到大图像的上采样。这就是我们要实现的目标。现在，您还可以看到“转置卷积”这个名称的来源。

## 反卷积

将经过卷积的图像逆推还原，显示原始图像就是反卷积所做的操作。

## 区别

将5x5图像经过步长为2的3x3卷积，将会生成2x2的图像。

### 转置卷积

将2\*2的图像进行一些填充，之后再进行卷积，可以将图像的放大与卷积结合在一起，而不必执行两个单独的过程。

### 反卷积

对刚才的卷积操作进行逆操作，还原出5\*5图像

